# Збір даних зі сторінок

Ще одне важливе застосуванняя програмування — це&nbsp;збір інформації, наприклад з&nbsp;інтернету. Створюються програми, які називаються **ботами**, які відвідують різні вебсайти та&nbsp;збирають дані. Такий бот міг би, наприклад, відвідувати певний маркетплейс і збирати інформацію про ціни на&nbsp;різні товари, а потім порівнювати їх із цінами в&nbsp;інших місцях або переглядати, як&nbsp;вони змінюються з&nbsp;часом. Такі боти часто пишуться на&nbsp;Python.

## Канал RSS

Погляньмо, як&nbsp;може виглядати такий збір даних. Перегляньмо заголовки блогів і новинних вебсайтів. Я почну з&nbsp;популярного блогу *Wait but why*.

{width: 80%}
![](404_wait_but_why.png)

Назви заголовків можна було б взяти безпосередньо з&nbsp;домашньої сторінки, але дозволь мені поділитися дещо простішим прийомом. У більшості блогів є сторінка під назвою канал RSS (RSS feed), яка надає перелік останніх публікацій. Ми&nbsp;використаємо це, після чого зчитаємо заголовки статей за допомогою пакета `feedparser`.

```python
import feedparser

rss = feedparser.parse("http://waitbutwhy.com/rss")
entry = rss.entries[1]
titles = [feed.title for feed in rss.entries]
for title in titles:
    print(title)
# Mailbag #2
# The Big and the Small
# You Won’t Believe My Morning
# ...
```

Так само ми&nbsp;могли б зчитати їхній зміст або посилання на&nbsp;зображення, які ілюструють ці статті.

## Зчитування змісту сторінки

RSS-сторінки зазвичай мають обмеження на&nbsp;кількість представлених публікацій. Тому, якщо ми&nbsp;хочемо завантажити більше заголовків або взяти заголовки з&nbsp;рейтингу найпопулярніших статей, нам доведеться робити якісь операції зі сторінкою. Я покажу це&nbsp;на&nbsp;прикладі *positive.news* — вебсайту, який, на&nbsp;відміну від більшості ЗМІ, зосереджується на&nbsp;позитивних світових новинах.

Тут ми&nbsp;використаємо пакети: `urlopen` для завантаження вмісту вебсайту, та&nbsp;`BeautifulSoup` для пошуку конкретних елементів на&nbsp;цій сторінці. Я скористався опцією "Перевірити" в&nbsp;Google Chrome і виявив, що&nbsp;заголовки містяться в&nbsp;елементах типу `а`, з&nbsp;класом `card__title` [^403_1].

![](404_positivenews.png)

Завдяки цьому їх легко знайти та&nbsp;відобразити.

```python
from bs4 import BeautifulSoup
from urllib.request import urlopen

html = urlopen("https://www.positive.news/").read()
soup = BeautifulSoup(html, "html.parser")
titles_elem = soup\
    .find_all("a", {"class": "card__title"})
for title in titles_elem:
    print(title.text)
# What went right this week: Wales’ ‘basic income’, ...
# The bicycles that grow on trees
# The plan to recycle Britain’s waste glass, ...
```

За допомогою цих інструментів ми&nbsp;могли б, наприклад, написати програму, яка час&nbsp;від часу перевірятиме, чи з’явилася нова стаття на&nbsp;сайтах, які нас цікавлять.

У практичних застосуваннях програми для збору даних дещо складніші. Вони постійно досліджують різні сторінки в&nbsp;пошуках потрібної інформації. Однак це&nbsp;просто більш розбудоване використання того, що&nbsp;ми&nbsp;здебільшого вже бачили.

{pagebreak}

## REST клієнт

Коли ми&nbsp;користуємося Facebook, Twitter чи Instagram, ми&nbsp;відвідуємо їхні вебсайти. Ви колись задумувалися, як&nbsp;вони спілкуються один з&nbsp;одним? Наприклад, у&nbsp;Twitter можна увійти за допомогою Google, або, публікуючи допис в&nbsp;Instagram, можна налаштувати автоматичну публікацію у&nbsp;Twitter. Важко уявити, щоб пристрій, з&nbsp;якого ти користуєшся Instagram, заходив замість тебе у&nbsp;Twitter. Повідомлення з&nbsp;одного сервісу іншому надходить за допомогою API.

API (скорочено від Application Programming Interface) — це&nbsp;набір правил, які описують, як&nbsp;комп’ютери або програми взаємодіють одне з&nbsp;одним. У випадку вебсайтів це&nbsp;спілкування найчастіше здійснюється шляхом надсилання один одному повідомлень із певною структурою.

Завдяки простоті створення та&nbsp;поширення пакетів ми&nbsp;можемо легко використовувати різні API. У PyPI є пакети, які дозволяють нам спілкуватися з&nbsp;Twitter, Facebook, Instagram та&nbsp;багатьма іншими платформами. Я навіть знайшов пакет для зв’язку з&nbsp;API, який повертає інформацію про покемонів.

```python
import pokebase as pb

charizard = pb.pokemon("charizard")
print(charizard.name)  # charizard
print(charizard.height)  # 17
types = [t.type.name for t in charizard.types]
print(types)  # ['fire', 'flying']
```

API для завантаження покемонів публічний, що&nbsp;означає, що&nbsp;він не&nbsp;потребує підтвердження нашої особи. Дещо інакше працює API для Twitter, де&nbsp;дуже важливо переконатися, що&nbsp;код має повноваження виконувати певні дії. Адже один допис відомої людини може серйозно нашкодити її&nbsp;іміджу. Тому отримати такі повноваження нелегко. Я пропущу кроки реєстрації та&nbsp;автентифікації особи. З точки зору програмування для відправлення твіту достатньо такого коду:

```python
import tweepy

consumer_key = "<Приховано>"
consumer_secret = "< Приховано >"
access_token = "< Приховано >"
access_token_secret = "< Приховано>"
auth = tweepy.OAuthHandler(
    consumer_key,
    consumer_secret
)
auth.set_access_token(
    access_token,
    access_token_secret
)
api = tweepy.API(auth)

status = api.update_status(status="Hello, World!")
```

{width: 60%}
![](tweet.png)

Як бачиш, це&nbsp;дуже просто. Аналогічно з&nbsp;багатьма іншими порталами. Це цінують програмісти, які займаються автоматизацією процесів. Наприклад, плануванням та&nbsp;публікацією твітів у&nbsp;маркетингових агентствах.

## Завершення

У цьому розділі ми&nbsp;ознайомилися з&nbsp;різними методами спілкування з&nbsp;вебсайтами, які допомагають збирати дані або автоматизувати процеси. Вони широко використовуються багатьма компаніями. І не&nbsp;лише програмістами, а й, перш за все, тестувальниками, адміністраторами та&nbsp;дослідниками. Також я зустрічав багато технічних менеджерів, які пишуть боти, щоб спростити свою роботу. Наприклад, замість того, щоб кожного дня перевіряти результати продажів, можна написати програму, яка робитиме це&nbsp;сама і додаватиме дані до документа, який створюватиме на&nbsp;їхній основі графік. Тож це&nbsp;дуже універсальні інструменти, про які точно слід знати.

[^403_1]: Для такого використання Python нам знадобиться знання HTML, тобто мови опису вебсторінок.



